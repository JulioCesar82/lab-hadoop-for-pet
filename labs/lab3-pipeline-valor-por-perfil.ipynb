{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Pipeline Interativo de Cálculo de Valor por Perfil de Pet\n",
    "\n",
    "Este notebook executa o pipeline de cálculo de Valor por Perfil de Pet (LTV). As células abaixo irão compilar o código MapReduce em Java, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop e verificar os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compilando a Aplicação MapReduce\n",
    "\n",
    "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd mapreduce-jobs/ltv-by-pet-profile\n",
    "mvn package -q\n",
    "echo \"JAR compilado com sucesso em: target/ltv-by-pet-profile-1.0-SNAPSHOT.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingestão de Dados com Sqoop\n",
    "\n",
    "Agora, vamos importar os dados de compras do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros.\n",
    "\n",
    "A query Sqoop já realiza um JOIN para obter o perfil do pet e o valor da compra, simplificando o MapReduce posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_ltv\n",
    "\n",
    "# Remove o diretório de entrada se ele já existir\n",
    "hdfs dfs -test -d $INPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
    "    hdfs dfs -rm -r $INPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Iniciando importação com Sqoop...\"\n",
    "sqoop import \\\n",
    "    --connect jdbc:postgresql://postgres:5432/petshop_db \\\n",
    "    --username petshop_user \\\n",
    "    --password petshop_password \\\n",
    "    --query \"SELECT \\\n",
    "                CONCAT(p.especie, ';', p.tipo_pelo) AS perfil_pet, \\\n",
    "                (hc.quantidade * pr.preco) AS valor_compra \\\n",
    "            FROM  compra hc \\\n",
    "            JOIN produto pr ON hc.produto_id = pr.produto_id \\\n",
    "            JOIN pet p ON hc.tutor_id = p.tutor_id \\\n",
    "            WHERE \\$CONDITIONS\" \\\n",
    "    --target-dir $INPUT_DIR \\\n",
    "    --split-by hc.compra_id \\\n",
    "    --m 1\n",
    "\n",
    "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
    "hdfs dfs -ls $INPUT_DIR\n",
    "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execução do Job MapReduce\n",
    "\n",
    "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o `.jar` que compilamos para o Hadoop. O resultado será salvo no diretório `/petshop/output_ltv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_ltv\n",
    "OUTPUT_DIR=/petshop/output_ltv\n",
    "JAR_PATH=/home/jovyan/work/labs/mapreduce-jobs/ltv-by-pet-profile/target/ltv-by-pet-profile-1.0-SNAPSHOT.jar\n",
    "\n",
    "# Remove o diretório de saída se ele já existir\n",
    "hdfs dfs -test -d $OUTPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
    "    hdfs dfs -rm -r $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Executando o job MapReduce...\"\n",
    "hadoop jar $JAR_PATH com.petshop.hadoop.LTVCalculation $INPUT_DIR $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nJob concluído!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verificação do Resultado\n",
    "\n",
    "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `perfil_pet` e o `valor_total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIR=/petshop/output_ltv\n",
    "echo \"Conteúdo do diretório de saída:\"\n",
    "hdfs dfs -ls $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nResultado do processamento:\"\n",
    "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
