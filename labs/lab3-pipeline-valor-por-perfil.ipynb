{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define qual implementação de MapReduce usar: 'java' ou 'python'\n",
                "MAPREDUCE_LANG = 'java' # Altere para 'python' para usar a implementação em Python\n",
                "\n",
                "print(f\"Usando implementação de MapReduce: {MAPREDUCE_LANG.upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 3: Pipeline Interativo de Cálculo de Valor por Perfil de Pet\n",
                "\n",
                "Este notebook executa o pipeline de cálculo de Valor por Perfil de Pet (LTV). As células abaixo irão compilar o código MapReduce em Java, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop e verificar os resultados."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Compilando a Aplicação MapReduce\n",
                "\n",
                "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
                "    cd ltv-by-pet-profile\n",
                "    mvn package -q\n",
                "    echo \"JAR compilado com sucesso em: target/ltv-by-pet-profile-1.0-SNAPSHOT.jar\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Origem dos dados\n",
                "\n",
                "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` com os dados inseridos anteriormente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "\n",
                "# As credenciais e o host são baseados no arquivo docker-compose.txt\n",
                "DB_HOST = \"localhost\" # Nome do serviço no Docker Compose\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "try:\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "    \n",
                "    cur.execute(\"SELECT CONCAT(p.species, ';', p.fur_type) AS perfil_pet, (hc.quantity * pr.price) AS valor_compra FROM  purchase hc JOIN product pr ON hc.product_id = pr.product_id JOIN pet p ON hc.tutor_id = p.tutor_id\")\n",
                "    rows = cur.fetchall()\n",
                "    \n",
                "    print(\"Agendamentos encontrados:\")\n",
                "    for row in rows:\n",
                "        print(row)\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Ocorreu um erro: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Ingestão de Dados com Sqoop\n",
                "\n",
                "Agora, vamos importar os dados de compras do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros.\n",
                "\n",
                "A query Sqoop já realiza um JOIN para obter o perfil do pet e o valor da compra, simplificando o MapReduce posterior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "INPUT_DIR=/petshop/input_ltv\n",
                "\n",
                "# Remove o diretório de entrada se ele já existir\n",
                "hdfs dfs -test -d $INPUT_DIR\n",
                "if [ $? -eq 0 ]; then\n",
                "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
                "    hdfs dfs -rm -r $INPUT_DIR\n",
                "fi\n",
                "\n",
                "echo \"Iniciando importação com Sqoop...\"\n",
                "sqoop import \\\n",
                "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
                "    --username postgres \\\n",
                "    --password postgres \\\n",
                "    --query \"SELECT \\\n",
                "                CONCAT(p.species, ';', p.fur_type) AS perfil_pet, \\\n",
                "                (hc.quantity * pr.price) AS valor_compra \\\n",
                "            FROM  purchase hc \\\n",
                "            JOIN product pr ON hc.product_id = pr.product_id \\\n",
                "            JOIN pet p ON hc.tutor_id = p.tutor_id \\\n",
                "            WHERE \\$CONDITIONS\" \\\n",
                "    --target-dir $INPUT_DIR \\\n",
                "    --m 1 \\\n",
                "    --split-by hc.purchase_id\n",
                "\n",
                "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
                "hdfs dfs -ls $INPUT_DIR\n",
                "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Execução do Job MapReduce\n",
                "\n",
                "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o `.jar` que compilamos para o Hadoop. O resultado será salvo no diretório `/petshop/output_ltv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "INPUT_DIR=/petshop/input_ltv\n",
                "OUTPUT_DIR=/petshop/output_ltv\n",
                "\n",
                "# Remove o diretório de saída se ele já existir\n",
                "hdfs dfs -test -d $OUTPUT_DIR\n",
                "if [ $? -eq 0 ]; then\n",
                "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
                "    hdfs dfs -rm -r $OUTPUT_DIR\n",
                "fi\n",
                "\n",
                "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
                "    JAR_PATH=/home/jovyan/work/labs/mapreduce-jobs/ltv-by-pet-profile/target/ltv-by-pet-profile-1.0-SNAPSHOT.jar\n",
                "   \n",
                "    echo \"Executando o job MapReduce (Java)...\n",
                "    hadoop jar $JAR_PATH $INPUT_DIR $OUTPUT_DIR\n",
                "\n",
                "elif [ \"$MAPREDUCE_LANG\" == \"python\" ]; then\n",
                "    MAPPER_PATH=labs/mapreduce-jobs/ltv-by-pet-profile-python/mapper.py\n",
                "    REDUCER_PATH=labs/mapreduce-jobs/ltv-by-pet-profile-python/reducer.py\n",
                " \n",
                "    echo \"Executando o job MapReduce (Python)...\n",
                "    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
                "        -file $MAPPER_PATH -mapper $MAPPER_PATH \\\n",
                "        -file $REDUCER_PATH -reducer $REDUCER_PATH \\\n",
                "        -input $INPUT_DIR \\\n",
                "        -output $OUTPUT_DIR\n",
                "fi\n",
                "\n",
                "echo \"\\nJob concluído!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Verificação do Resultado\n",
                "\n",
                "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `perfil_pet` e o `valor_total`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "OUTPUT_DIR=/petshop/output_ltv\n",
                "echo \"Conteúdo do diretório de saída:\"\n",
                "hdfs dfs -ls $OUTPUT_DIR\n",
                "\n",
                "echo \"\\nResultado do processamento:\"\n",
                "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
