{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# Define qual implementação de MapReduce usar: 'java' ou 'python'\n",
                "MAPREDUCE_LANG = 'java'  # Altere para 'python' para usar a implementação em Python\n",
                "os.environ['MAPREDUCE_LANG'] = MAPREDUCE_LANG\n",
                "\n",
                "print(f\"Usando implementação de MapReduce: {os.environ['MAPREDUCE_LANG'].upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2: Recomendação para ir ao Banho e Tosa\n",
                "\n",
                "Este notebook executa o pipeline de recomendação de ida ao banho e tosa de ponta a ponta. As células abaixo irão compilar o código MapReduce, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop, verifica os resultados e armazena os dados resultantes."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compilando a Aplicação MapReduce\n",
                "\n",
                "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
                "    cd booking-recommendation\n",
                "    mvn package -q\n",
                "    echo \"JAR compilado com sucesso em: target/booking-recommendation-1.0-SNAPSHOT.jar\"\n",
                "fi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Consultando histórico de Agendamentos\n",
                "\n",
                "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` com os dados inseridos anteriormente."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "\n",
                "# As credenciais e o host são baseados no arquivo docker-compose.txt\n",
                "DB_HOST = \"localhost\" # Nome do serviço no Docker Compose\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "try:\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "    \n",
                "    cur.execute(\"\"\"SELECT pet.pet_id, pet.name, booking.booking_date \n",
                "                FROM booking \n",
                "                JOIN pet ON booking.pet_id = pet.pet_id\n",
                "                WHERE booking.status = 'Realizado'\"\"\")\n",
                "    rows = cur.fetchall()\n",
                "    \n",
                "    print(\"Agendamentos encontrados:\")\n",
                "    for row in rows:\n",
                "        print(row)\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Ocorreu um erro: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Montar frequência média com base no histórico de Agendamentos de todos os Pets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Ingestão do histórico de Agendamentos com Sqoop\n",
                "\n",
                "Importar os dados de agendamentos do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "INPUT_DIR=/petshop/input_booking_recommendation\n",
                "\n",
                "# Remove o diretório de entrada se ele já existir\n",
                "hdfs dfs -test -d $INPUT_DIR\n",
                "if [ $? -eq 0 ]; then\n",
                "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
                "    hdfs dfs -rm -r $INPUT_DIR\n",
                "fi\n",
                "\n",
                "echo \"Iniciando importação com Sqoop...\"\n",
                "sqoop import \\\n",
                "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
                "    --username postgres \\\n",
                "    --password postgres \\\n",
                "    --query \"SELECT pet.pet_id, pet.name, booking.booking_date \\\n",
                "            FROM booking \\\n",
                "            JOIN pet ON booking.pet_id = pet.pet_id \\\n",
                "            WHERE booking.status = 'Realizado' AND \\$CONDITIONS\" \\\n",
                "    --target-dir $INPUT_DIR \\\n",
                "    --m 1 \\\n",
                "    --split-by pet.pet_id\n",
                "\n",
                "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
                "hdfs dfs -ls $INPUT_DIR\n",
                "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Execução do Job MapReduce\n",
                "\n",
                "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o código Java ou Python para o Hadoop. O resultado será salvo no diretório `/petshop/output_booking_recommendation`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "INPUT_DIR=/petshop/input_booking_recommendation\n",
                "OUTPUT_DIR=/petshop/output_booking_recommendation\n",
                "\n",
                "# Remove o diretório de saída se ele já existir\n",
                "hdfs dfs -test -d $OUTPUT_DIR\n",
                "if [ $? -eq 0 ]; then\n",
                "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
                "    hdfs dfs -rm -r $OUTPUT_DIR\n",
                "fi\n",
                "\n",
                "if [ \"$MAPREDUCE_LANG\" == \"java\" ]; then\n",
                "    JAR_PATH=booking-recommendation/target/booking-recommendation-1.0-SNAPSHOT.jar\n",
                "   \n",
                "    echo \"Executando o job MapReduce (Java)...\"\n",
                "    hadoop jar $JAR_PATH -D log4j.logger.com.petshop.hadoop.FrequencyMapper=DEBUG $INPUT_DIR $OUTPUT_DIR\n",
                "\n",
                "elif [ \"$MAPREDUCE_LANG\" == \"python\" ]; then\n",
                "    MAPPER_PATH=booking-recommendation-python/mapper.py\n",
                "    REDUCER_PATH=booking-recommendation-python/reducer.py\n",
                "  \n",
                "    echo \"Executando o job MapReduce (Python)...\"\n",
                "    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
                "        -file $MAPPER_PATH -mapper $MAPPER_PATH \\\n",
                "        -file $REDUCER_PATH -reducer $REDUCER_PATH \\\n",
                "        -input $INPUT_DIR \\\n",
                "        -output $OUTPUT_DIR\n",
                "fi\n",
                "\n",
                "echo \"\\nJob concluído!\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Verificando resultados\n",
                "\n",
                "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `pet_id`, a `suggested_date` e o `average_frequency_days`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "OUTPUT_DIR=/petshop/output_booking_recommendation\n",
                "\n",
                "echo \"Conteúdo do diretório de saída:\"\n",
                "hdfs dfs -ls $OUTPUT_DIR\n",
                "\n",
                "echo \"\\nResultado do processamento:\"\n",
                "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Gravando e consultando resultados no Redis\n",
                "\n",
                "A etapa final na arquitetura real seria carregar este resultado no Redis para consulta rápida. O serviço `loader-py` faria isso automaticamente. Abaixo, simulamos como os dados seriam armazenados e consultados usando `redis-cli`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import redis\n",
                "redis = redis.Redis(host = 'localhost', port=6379)\n",
                "redis.ping()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "OUTPUT_FILE=/petshop/output_booking_recommendation/part-r-00000\n",
                "REDIS_HOST=localhost\n",
                "\n",
                "# Verifica se o arquivo de resultado existe\n",
                "hdfs dfs -test -e $OUTPUT_FILE\n",
                "if [ $? -ne 0 ]; then\n",
                "    echo \"Erro: Arquivo de resultado não encontrado em $OUTPUT_FILE\"\n",
                "    exit 1\n",
                "fi\n",
                "\n",
                "echo \"Limpando recomendações antigas no Redis...\"\n",
                "redis-cli -h $REDIS_HOST KEYS \"recommendation:booking:pet:*\" | xargs -r redis-cli -h $REDIS_HOST DEL\n",
                "\n",
                "echo \"Carregando novas recomendações do HDFS para o Redis...\"\n",
                "# Lê o arquivo do HDFS e processa linha por linha\n",
                "hdfs dfs -cat $OUTPUT_FILE | while IFS=$'\\t' read -r pet_id values; do\n",
                "    # Extrai os valores (data sugerida e frequência)\n",
                "    sug_date=$(echo $values | cut -d',' -f1)\n",
                "    avg_freq=$(echo $values | cut -d',' -f2)\n",
                "    \n",
                "    # Monta e executa o comando HSET para o Redis\n",
                "    echo \"Inserindo recomendação para o pet_id: $pet_id\"\n",
                "    redis-cli -h $REDIS_HOST HSET \"recommendation:booking:pet:$pet_id\" \\\n",
                "        suggested_date \"$sug_date\" \\\n",
                "        average_frequency_days \"$avg_freq\"\n",
                "done\n",
                "\n",
                "echo \"\\nCarga de dados concluída.\"\n",
                "\n",
                "echo \"Todos os registros inseridos:\"\n",
                "# redis-cli -h $REDIS_HOST KEYS \"recommendation:booking:pet:*\"\n",
                "\n",
                "for key in $(redis-cli -h $REDIS_HOST KEYS \"recommendation:booking:pet:*\");\n",
                "  do echo \"\\n Key : '$key'\" \n",
                "     redis-cli -h $REDIS_HOST HGETALL $key;\n",
                "done"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Gravando resultados no PostgreSQL\n",
                "\n",
                "Com o resultado processado e validado, a próxima etapa é armazená-lo no PostgreSQL para consumo por outras aplicações. A célula abaixo lê o resultado do HDFS e o insere na tabela `booking_recommendation`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import psycopg2\n",
                "import os\n",
                "from subprocess import Popen, PIPE\n",
                "\n",
                "# Database connection details\n",
                "DB_HOST = \"localhost\"\n",
                "DB_NAME = \"postgres\"\n",
                "DB_USER = \"postgres\"\n",
                "DB_USER_PWD = \"postgres\"\n",
                "\n",
                "# HDFS output file\n",
                "output_file = \"/petshop/output_booking_recommendation/part-r-00000\"\n",
                "\n",
                "try:\n",
                "    # Connect to the database\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "\n",
                "    # Truncate the table before inserting new data\n",
                "    cur.execute(\"TRUNCATE TABLE booking_recommendation;\")\n",
                "    print(\"Table booking_recommendation truncated.\")\n",
                "\n",
                "    # Read the HDFS file\n",
                "    process = Popen([\"hdfs\", \"dfs\", \"-cat\", output_file], stdout=PIPE)\n",
                "    (stdout, stderr) = process.communicate()\n",
                "    exit_code = process.wait()\n",
                "\n",
                "    if exit_code == 0:\n",
                "        # Decode the output and split into lines\n",
                "        lines = stdout.decode(\"utf-8\").strip().split('\\n')\n",
                "        \n",
                "        print(f\"Inserting {len(lines)} records into booking_recommendation...\")\n",
                "        \n",
                "        # Process each line and insert into the database\n",
                "        for line in lines:\n",
                "            if not line:\n",
                "                continue\n",
                "                \n",
                "            pet_id, values = line.split('\\t')\n",
                "            suggested_date, avg_freq_days = values.split(',')\n",
                "            \n",
                "            # Prepare and execute the INSERT statement\n",
                "            cur.execute(\n",
                "                \"INSERT INTO booking_recommendation (pet_id, suggested_date, average_frequency_days) VALUES (%s, %s, %s)\",\n",
                "                (int(pet_id), suggested_date, int(avg_freq_days))\n",
                "            )\n",
                "        \n",
                "        # Commit the transaction\n",
                "        conn.commit()\n",
                "        print(\"Data successfully inserted into booking_recommendation.\")\n",
                "\n",
                "    else:\n",
                "        print(f\"Error reading HDFS file: {stderr}\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Consultando dados inseridos no PostgreSQL\n",
                "\n",
                "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` para verificar se os dados foram inseridos corretamente na tabela `booking_recommendation`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
                "    cur = conn.cursor()\n",
                "    \n",
                "    cur.execute(\"SELECT * FROM booking_recommendation;\")\n",
                "    rows = cur.fetchall()\n",
                "    \n",
                "    print(\"Registros encontrados:\")\n",
                "    for row in rows:\n",
                "        print(row)\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"Ocorreu um erro: {e}\")\n",
                "finally:\n",
                "    if 'conn' in locals() and conn is not None:\n",
                "        cur.close()\n",
                "        conn.close()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
