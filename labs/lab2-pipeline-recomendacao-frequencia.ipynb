{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Pipeline Interativo de Recomendação de Frequência\n",
    "\n",
    "Este notebook executa o pipeline de recomendação de frequência de ponta a ponta. As células abaixo irão compilar o código MapReduce em Java, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop e verificar os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compilando a Aplicação MapReduce\n",
    "\n",
    "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd recommendation-frequency\n",
    "mvn package -q\n",
    "echo \"JAR compilado com sucesso em: target/recommendation-frequency-1.0-SNAPSHOT.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingestão de Dados com Sqoop\n",
    "\n",
    "Agora, vamos importar os dados de agendamentos do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_agendamentos\n",
    "\n",
    "# Remove o diretório de entrada se ele já existir\n",
    "hdfs dfs -test -d $INPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
    "    hdfs dfs -rm -r $INPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Iniciando importação com Sqoop...\"\n",
    "sqoop import \\\n",
    "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
    "    --username postgres \\\n",
    "    --password postgres \\\n",
    "    --query \"SELECT pet.pet_id, pet.nome, agendamento.data_agendamento FROM agendamento JOIN pet ON agendamento.pet_id = pet.pet_id WHERE agendamento.status = 'Realizado' AND \\$CONDITIONS\" \\\n",
    "    --target-dir $INPUT_DIR \\\n",
    "    --m 1 \\\n",
    "    --split-by pet.pet_id\n",
    "\n",
    "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
    "hdfs dfs -ls $INPUT_DIR\n",
    "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execução do Job MapReduce\n",
    "\n",
    "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o `.jar` que compilamos para o Hadoop. O resultado será salvo no diretório `/petshop/output_recomendacoes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_agendamentos\n",
    "OUTPUT_DIR=/petshop/output_recomendacoes\n",
    "JAR_PATH=recommendation-frequency/target/recommendation-frequency-1.0-SNAPSHOT.jar\n",
    "\n",
    "# Remove o diretório de entrada se ele já existir (para evitar FileAlreadyExistsException se for usado como output)\n",
    "hdfs dfs -test -d $INPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS de entrada existente: $INPUT_DIR\"\n",
    "    hdfs dfs -rm -r $INPUT_DIR\n",
    "fi\n",
    "\n",
    "# Remove o diretório de saída se ele já existir\n",
    "hdfs dfs -test -d $OUTPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
    "    hdfs dfs -rm -r $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Executando o job MapReduce...\"\n",
    "hadoop jar $JAR_PATH com.petshop.hadoop.FrequencyRecommendation $INPUT_DIR $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nJob concluído!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verificação do Resultado\n",
    "\n",
    "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `pet_id`, a `data_sugerida` e a `frequencia_media`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIR=/petshop/output_recomendacoes\n",
    "echo \"Conteúdo do diretório de saída:\"\n",
    "hdfs dfs -ls $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nResultado do processamento:\"\n",
    "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Carregando e Consultando no Redis (Exemplo)\n",
    "\n",
    "A etapa final na arquitetura real seria carregar este resultado no Redis para consulta rápida. O serviço `loader-py` faria isso automaticamente. Abaixo, simulamos como os dados seriam armazenados e consultados usando `redis-cli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# O nome 'redis' funciona porque os contêineres estão na mesma rede Docker\n",
    "redis-cli -h redis HSET recomendacao:pet:1 pet_nome \"Bidu\" data_sugerida \"2025-07-13\" frequencia_media_dias \"31\"\n",
    "redis-cli -h redis EXPIRE recomendacao:pet:1 604800\n",
    "\n",
    "echo \"Dados de recomendação para o pet 1 armazenados no Redis.\"\n",
    "echo \"Consultando os dados:\"\n",
    "redis-cli -h redis HGETALL recomendacao:pet:1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
