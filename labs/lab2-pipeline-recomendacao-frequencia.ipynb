{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Pipeline Interativo de Recomendação de Frequência\n",
    "\n",
    "Este notebook executa o pipeline de recomendação de frequência de ponta a ponta. As células abaixo irão compilar o código MapReduce em Java, ingerir dados do PostgreSQL com Sqoop, executar o job no Hadoop e verificar os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compilando a Aplicação MapReduce\n",
    "\n",
    "A primeira etapa é compilar nosso código-fonte Java em um arquivo `.jar` executável. A célula abaixo usa o Maven para isso. Ela navega até o diretório do projeto e executa o `mvn package`. A flag `-q` é para uma saída mais limpa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd recommendation-frequency\n",
    "mvn package -q\n",
    "echo \"JAR compilado com sucesso em: target/recommendation-frequency-1.0-SNAPSHOT.jar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Origem dos dados\n",
    "\n",
    "Execute a célula abaixo para se conectar novamente e fazer uma consulta `SELECT` com os dados inseridos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# As credenciais e o host são baseados no arquivo docker-compose.txt\n",
    "DB_HOST = \"localhost\" # Nome do serviço no Docker Compose\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_USER_PWD = \"postgres\"\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(host=DB_HOST, dbname=DB_NAME, user=DB_USER, password=DB_USER_PWD)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute(\"SELECT pet.pet_id, pet.nome, agendamento.data_agendamento FROM agendamento JOIN pet ON agendamento.pet_id = pet.pet_id WHERE agendamento.status = 'Realizado'\")\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    print(\"Agendamentos encontrados:\")\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro: {e}\")\n",
    "finally:\n",
    "    if 'conn' in locals() and conn is not None:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingestão de Dados com Sqoop\n",
    "\n",
    "Agora, vamos importar os dados de agendamentos do PostgreSQL para o HDFS. O comando `hdfs dfs -rm` é usado para remover o diretório de destino antes da importação, garantindo que possamos executar esta célula várias vezes sem erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_agendamentos\n",
    "\n",
    "# Remove o diretório de entrada se ele já existir\n",
    "hdfs dfs -test -d $INPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS existente: $INPUT_DIR\"\n",
    "    hdfs dfs -rm -r $INPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Iniciando importação com Sqoop...\"\n",
    "sqoop import \\\n",
    "    --connect jdbc:postgresql://localhost:5432/postgres \\\n",
    "    --username postgres \\\n",
    "    --password postgres \\\n",
    "    --query \"SELECT pet.pet_id, pet.nome, agendamento.data_agendamento FROM agendamento JOIN pet ON agendamento.pet_id = pet.pet_id WHERE agendamento.status = 'Realizado' AND \\$CONDITIONS\" \\\n",
    "    --target-dir $INPUT_DIR \\\n",
    "    --m 1 \\\n",
    "    --split-by pet.pet_id\n",
    "\n",
    "echo \"\\nImportação concluída. Verificando os dados no HDFS:\"\n",
    "hdfs dfs -ls $INPUT_DIR\n",
    "hdfs dfs -cat $INPUT_DIR/part-m-00000 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execução do Job MapReduce\n",
    "\n",
    "Com os dados no HDFS, podemos executar nosso job MapReduce. A célula abaixo submete o `.jar` que compilamos para o Hadoop. O resultado será salvo no diretório `/petshop/output_recomendacoes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "INPUT_DIR=/petshop/input_agendamentos\n",
    "OUTPUT_DIR=/petshop/output_recomendacoes\n",
    "JAR_PATH=recommendation-frequency/target/recommendation-frequency-1.0-SNAPSHOT.jar\n",
    "\n",
    "# Remove o diretório de saída se ele já existir\n",
    "hdfs dfs -test -d $OUTPUT_DIR\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Removendo diretório HDFS de saída existente: $OUTPUT_DIR\"\n",
    "    hdfs dfs -rm -r $OUTPUT_DIR\n",
    "fi\n",
    "\n",
    "echo \"Executando o job MapReduce...\"\n",
    "hadoop jar $JAR_PATH -D log4j.logger.com.petshop.hadoop.FrequencyMapper=DEBUG $INPUT_DIR $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nJob concluído!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verificação do Resultado\n",
    "\n",
    "Vamos verificar o resultado processado no HDFS. O arquivo `part-r-00000` deve conter o `pet_id`, a `data_sugerida` e a `frequencia_media`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_DIR=/petshop/output_recomendacoes\n",
    "echo \"Conteúdo do diretório de saída:\"\n",
    "hdfs dfs -ls $OUTPUT_DIR\n",
    "\n",
    "echo \"\\nResultado do processamento:\"\n",
    "hdfs dfs -cat $OUTPUT_DIR/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Carregando e Consultando no Redis (Exemplo)\n",
    "\n",
    "A etapa final na arquitetura real seria carregar este resultado no Redis para consulta rápida. O serviço `loader-py` faria isso automaticamente. Abaixo, simulamos como os dados seriam armazenados e consultados usando `redis-cli`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "redis = redis.Redis(host = 'localhost', port=6379)\n",
    "redis.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTPUT_FILE=/petshop/output_recomendacoes/part-r-00000\n",
    "REDIS_HOST=localhost\n",
    "\n",
    "# Verifica se o arquivo de resultado existe\n",
    "hdfs dfs -test -e $OUTPUT_FILE\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"Erro: Arquivo de resultado não encontrado em $OUTPUT_FILE\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"Limpando recomendações antigas no Redis...\"\n",
    "redis-cli -h $REDIS_HOST KEYS \"recomendacao:pet:*\" | xargs -r redis-cli -h $REDIS_HOST DEL\n",
    "\n",
    "echo \"Carregando novas recomendações do HDFS para o Redis...\"\n",
    "# Lê o arquivo do HDFS e processa linha por linha\n",
    "hdfs dfs -cat $OUTPUT_FILE | while IFS=$'\\t' read -r pet_id values; do\n",
    "    # Extrai os valores (data e frequência)\n",
    "    sug_date=$(echo $values | cut -d',' -f1)\n",
    "    avg_freq=$(echo $values | cut -d',' -f2)\n",
    "    \n",
    "    # Monta e executa o comando HSET para o Redis\n",
    "    echo \"Inserindo recomendação para o pet_id: $pet_id\"\n",
    "    redis-cli -h $REDIS_HOST HSET \"recomendacao:pet:$pet_id\" \\\n",
    "        data_sugerida \"$sug_date\" \\\n",
    "        frequencia_media_dias \"$avg_freq\"\n",
    "done\n",
    "\n",
    "echo \"\\nCarga de dados concluída.\"\n",
    "\n",
    "echo \"Totos os registros inseridos:\"\n",
    "redis-cli -h $REDIS_HOST KEYS \"recomendacao:pet:*\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
